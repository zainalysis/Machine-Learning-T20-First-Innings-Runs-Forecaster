# -*- coding: utf-8 -*-
"""T20_First_Innings_Total_Runs_Predictor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XyL5oeUtD92HB9RStAxMkgCCrtpp_wd0
"""

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

import pandas as pd

file_path = '/content/drive/My Drive/modelpred.csv'
df = pd.read_csv(file_path)

df.head()

"""#Selecting the required teams to avoid skewness"""

import pandas as pd


df = pd.read_csv(file_path)

# Define the set of teams
teams = {'Australia', 'Bangladesh', 'Afghanistan', 'India', 'England',
         'New Zealand', 'South Africa', 'Sri Lanka', 'Pakistan',
         'West Indies', 'Zimbabwe', 'Ireland', 'Scotland'}

# Filter the DataFrame based on the conditions
df = df[
    (df['inns'] == 1) &  # inns value should be 1
    (df['team_bat'].isin(teams)) &  # team_bat should be in the specified set
    (df['team_bowl'].isin(teams)) &  # team_bowl should be in the specified set
    (df['competition'] == 'T20I')  # competition should be 'T20I'
]

# Show the first few rows of the filtered DataFrame
df.head()

"""#Finding Grounds which have atleast hold 5 matches

"""

df['ground'].value_counts()[df['ground'].value_counts() > 600].sum()

eligible_grounds = list(df['ground'].value_counts()[df['ground'].value_counts() > 600].index)
eligible_grounds

df[df['ground'].isin(eligible_grounds)]

df

# Assuming your data is in a DataFrame named 'df'
# Calculate 'balls_bowled' as the difference between 120 and 'inns_balls_rem'
df['balls_bowled'] = 120 - df['inns_balls_rem']

# Verify the result
print(df[['inns_balls_rem', 'balls_bowled']].head())

import pandas as pd

# Assuming your data is in a DataFrame named 'df'
# Sort the DataFrame by 'p_match' and 'ball_id' to ensure correct rolling calculation
df = df.sort_values(by=['p_match', 'ball_id'])

# Define a function to calculate rolling sum for Last5OversRuns
def calculate_last5oversruns(group):
    # Initialize 'Last5OversRuns' column as NaN for first 30 rows
    group['Last5OversRuns'] = group['score'].rolling(window=30).sum()
    return group

# Apply the function grouped by 'p_match'
df = df.groupby('p_match').apply(calculate_last5oversruns)

# Reset the index to avoid index issues after groupby
df = df.reset_index(drop=True)

# Verify the result
print(df[['p_match', 'ball_id', 'balls_bowled', 'score', 'Last5OversRuns']].head(50))

df[['balls_bowled', 'inns_wkts' , 'inns_runs', 'Last5OversRuns']].head(50)

"""#Finding Total Innings Runs Per p_match (match)"""

# Group by 'p_match' and sum the 'score', then rename the 'score' column
grouped_df = df.groupby('p_match').sum()['score'].reset_index().rename(columns={'score': 'total_score'})

# Merge it with the original DataFrame
merged_df = df.merge(grouped_df, on='p_match')

# Display the merged DataFrame
print(merged_df.head())

df = merged_df
df

"""#Checking Basic Stats"""

df[['balls_bowled', 'inns_wkts' , 'inns_runs', 'total_score_x', 'Last5OversRuns']].head(50)

df.rename(columns={'total_score_x': 'total_score'}, inplace=True)

"""#Final Data Frame"""

df_final = df[['team_bat', 'team_bowl', 'ground', 'balls_bowled', 'inns_wkts' , 'inns_runs', 'Last5OversRuns', 'total_score' ]]
df_final

df_final.isnull().sum(  )

"""#Training"""

X = df_final.drop(columns=['total_score'])
y = df_final['total_score']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

X_train

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import r2_score, mean_absolute_error

trf = ColumnTransformer([
    ('trf',OneHotEncoder(drop='first'),['team_bat','team_bowl','ground'])
]
,remainder='passthrough')

pipe = Pipeline(steps=[
    ('step1', trf),
    ('step2', StandardScaler(with_mean=False)),  # Set with_mean=False here
    ('step3', XGBRegressor(n_estimators=1000, learning_rate=0.2, max_depth=12, random_state=1))
])

# Fitting the pipeline with training data
pipe.fit(X_train, y_train)

# Predicting the test data
y_pred = pipe.predict(X_test)

# Calculating the R^2 score
print(r2_score(y_test, y_pred))

# Calculating the Mean Absolute Error (MAE)
print(mean_absolute_error(y_test, y_pred))

import pickle

pickle.dump(pipe,open('pipe.pkl','wb'))